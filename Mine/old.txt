
# --- dimensionality reduction --- #
# LLE = partial(manifold.LocallyLinearEmbedding,
#               eigen_solver='dense',
#               neighbors_algorithm='auto',
#               random_state=42)
# methods = OrderedDict()
# methods['RAW'] = None
# nb_components_pca = list(range(0, data_train_test.shape[1], 5))
# for i in nb_components_pca:
#      pca = PCA(n_components=i)
#      X_train_pca = pca.fit_transform(X_train)
#      print('Total Explained Variance Ratio using {} components = {}%'.format(i, round(np.sum(pca.explained_variance_ratio_)*100, 2)))
# methods['PCA'] = PCA(n_components=35)
# nb_neighbors = 25
# nb_components = 20
# methods['LLE'] = LLE(n_neighbors=nb_neighbors, n_components=nb_components, method="standard")
# methods['MLLE'] = LLE(n_neighbors=nb_neighbors, n_components=nb_components, method="modified")
# elapsed_dr = OrderedDict()
# X_train_dict = OrderedDict()
# X_test_dict = OrderedDict()
# labels_dr = ['RAW', 'PCA', 'LLE', 'MLLE']
# for label in labels_dr:
#     if label == 'RAW':
#         X_train_dict[label] = X_train
#         X_test_dict[label] = X_test
#         elapsed_dr['RAW'] = 0
#         continue
#     start_time = time.time()
#     X_train_dict[label] = methods[label].fit_transform(X_train)
#     X_test_dict[label] = methods[label].transform(X_test)
#     elapsed_time = time.time() - start_time
#     elapsed_dr[label] = elapsed_time
#     print(label + ' finished in ' + f'{elapsed_time:.2f}' + ' s!')
# # set-up lle vs mlle figure
# list_comb = list(range(nb_components))
# list_comb = list(combinations(list_comb, 2))
# nb_pairs = min(len(list_comb), 5)
# list_comb = list_comb[:nb_pairs]
# fig, axs = plt.subplots(nb_pairs, 2, squeeze=False, figsize=(35, 18))
# fig.suptitle('Manifold Learning with %i neighbors and %i embeddings' % (nb_neighbors, nb_components), fontsize=14)
# for m, label in enumerate(labels_dr):
#     if label == 'PCA' or label == 'RAW':
#         continue
#     train = X_train_dict[label]
#     n = m - 2
#     for (l, x) in enumerate(list_comb):
#         axs[l, n].scatter(train[y_train == 0, x[0]],
#                           train[y_train == 0, x[1]], c='green', label='Survived')
#         axs[l, n].scatter(train[y_train == 1, x[0]],
#                           train[y_train == 1, x[1]], c='red', label='Died')
#         if l == 0:
#             axs[l, n].set_title('%s (%.2g sec)' % (label, elapsed_dr[label]))
#         axs[l, n].xaxis.set_major_formatter(NullFormatter())
#         axs[l, n].yaxis.set_major_formatter(NullFormatter())
#         axs[l, n].axis('tight')
#         axs[l, n].legend()
#         axs[l, n].set_xlabel(f'dim : {x[0]}')
#         axs[l, n].set_ylabel(f'dim : {x[1]}')
# fig.savefig('LLE_MLLE.png')

nb_neighbors = np.arange(3, 6, 2)
# print(nb_neighbors)
mesh = np.zeros((nb_neighbors[-1], 1))
# print(len(nb_neighbors), nb_neighbors[-1])
for nb_neighbor in nb_neighbors:
    comp = np.arange(1, nb_neighbor-1, 1)
    comp = np.append(comp, np.zeros(nb_neighbors[-1] - len(comp)))
    comp = np.reshape(comp, (len(comp), 1))
    mesh = np.hstack((mesh, comp))
mesh = mesh[:, 1:]
mesh = mesh.astype(int)
# print(mesh)
neighbors_algorithm = ['ball_tree', 'kd_tree', 'brute']
methods = OrderedDict()
elapsed_dr = OrderedDict()
X_train_dict = OrderedDict()
X_test_dict = OrderedDict()
labels_dr = []
for i in range(len(nb_neighbors)):
    temp = mesh[:, i]
    for j in range(temp.shape[0]):
        if temp[j] == 0:
            break
        for k in neighbors_algorithm:
            labels_dr.append('LLE:' + str(nb_neighbors[i]) + ':' + str(temp[j]) + ':' + k)
            labels_dr.append('MLLE:' + str(nb_neighbors[i]) + ':' + str(temp[j]) + ':' + k)
LLE = partial(manifold.LocallyLinearEmbedding,
              eigen_solver='dense',
              random_state=42)
for label in labels_dr:
    if label.split(':')[0] == 'LLE':
        methods[label] = LLE(n_neighbors=int(label.split(':')[1]), n_components=int(label.split(':')[2]), method="standard", neighbors_algorithm=label.split(':')[3])
    elif label.split(':')[0] == 'MLLE':
        methods[label] = LLE(n_neighbors=int(label.split(':')[1]), n_components=int(label.split(':')[2]), method="modified", neighbors_algorithm=label.split(':')[3])
    start_time = time.time()
    X_train_dict[label] = methods[label].fit_transform(X_train)
    X_test_dict[label] = methods[label].transform(X_test)
    elapsed_time = time.time() - start_time
    elapsed_dr[label] = elapsed_time
    print(label + ' finished in ' + f'{elapsed_time:.2f}' + ' s!')

# --- set-up classification --- #
# classifiers = OrderedDict()
# classifiers['Linear_SVM'] = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, C=1,
#                                       fit_intercept=False, intercept_scaling=1,
#                                       class_weight='balanced', max_iter=1000, random_state=42)
# classifiers['RBF_SVM'] = SVC(C=1, kernel='rbf', gamma=0.5, probability=False, class_weight='balanced')
# classifiers['KNN'] = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', metric='minkowski',
#                                           p=2, metric_params=None)
# classifiers['GMM'] = GaussianMixture(n_components=2, covariance_type='full', init_params='kmeans')
# labels_clf = ['Linear_SVM', 'RBF_SVM', 'KNN', 'GMM']
# elapsed_tot = []
# precision_scores = []
# recall_scores = []
# f1_scores = []
# accuracy_scores = []
# cms = []
# rocs = []
# states = []
# # # main loop
# for label_clf in labels_clf:
#     for label_dr in labels_dr:
#         start_time = time.time()
#         classifiers[label_clf].fit(X_train_dict[label_dr], y_train)
#         predictions = classifiers[label_clf].predict(X_test_dict[label_dr])
#         elapsed_time = time.time() - start_time
#         elapsed_tot.append(elapsed_time+elapsed_dr[label_dr])
#         states.append(label_clf + ':' + label_dr + ':' + f'{elapsed_tot[-1]:.2f}')
#         # precision_scores.append(metrics.precision_score(y_test, predictions, average='weighted'))
#         # recall_scores.append(metrics.recall_score(y_test, predictions, average='weighted'))
#         f1_scores.append(metrics.f1_score(y_test, predictions, average='weighted'))
#         accuracy_scores.append(metrics.accuracy_score(y_test, predictions))
#         cms.append(metrics.confusion_matrix(y_test, predictions, normalize='true'))
#         rocs.append(metrics.roc_curve(y_test, predictions))
#         print(label_clf + ' on ' + label_dr + ' finished in ' + f'{elapsed_time:.2f}' + ' s!')
# result = {'states': states, 'f1_scores': f1_scores, 'accuracy_scores': accuracy_scores,
#           'time': elapsed_tot, 'cms': cms, 'rocs': rocs}
# result_df = pd.DataFrame(data=result)
# result_df.to_csv('./results.csv')
# # # set-up figures
# result_df_reduced = result_df[['f1_scores', 'accuracy_scores']]
# ax = result_df_reduced.plot(kind='bar', figsize=(40, 40))
# ax.set_xticklabels(states, rotation=45, fontsize=20)
# plt.savefig('results.png')
# fig, axs = plt.subplots(len(labels_clf), len(labels_dr), squeeze=False, figsize=(40, 20))
# counter = 0
# for i in range(len(labels_clf)):
#     for j in range(len(labels_dr)):
#         sns.heatmap(cms[counter], annot=True, ax=axs[i, j])
#         axs[i, j].set_title(states[counter], fontsize=20)
#         counter = counter + 1
# fig.savefig('confusion_matrix.png')
# fig, axs = plt.subplots(len(labels_clf), len(labels_dr), squeeze=False, figsize=(40, 20))
# counter = 0
# for i in range(len(labels_clf)):
#     for j in range(len(labels_dr)):
#         fpr, tpr, _ = rocs[counter]
#         axs[i, j].plot(fpr, tpr)
#         axs[i, j].set_title(states[counter], fontsize=20)
#         counter = counter + 1
# fig.savefig('roc_curves.png')

classifiers = OrderedDict()
classifiers['RBF_SVM'] = SVC(C=1.0, kernel='rbf', gamma='scale', probability=False, class_weight='balanced')
labels_clf = ['RBF_SVM']
elapsed_tot = []
f1_scores = []
accuracy_scores = []
states = []
# # main loop
for label_clf in labels_clf:
    for label_dr in labels_dr:
        start_time = time.time()
        classifiers[label_clf].fit(X_train_dict[label_dr], y_train)
        predictions = classifiers[label_clf].predict(X_test_dict[label_dr])
        elapsed_time = time.time() - start_time
        elapsed_tot.append(elapsed_time+elapsed_dr[label_dr])
        states.append(label_clf + ':' + label_dr + ':' + f'{elapsed_tot[-1]:.2f}')
        f1_scores.append(metrics.f1_score(y_test, predictions, average='weighted'))
        accuracy_scores.append(metrics.accuracy_score(y_test, predictions))
        print(label_clf + ' on ' + label_dr + ' finished in ' + f'{elapsed_time:.2f}' + ' s!')
result_dr = {'states': states, 'f1_scores': f1_scores, 'accuracy_scores': accuracy_scores, 'time': elapsed_tot}
result_dr_df = pd.DataFrame(data=result_dr)
result_dr_df.to_csv('./result_dr.csv')
# # set-up figures
ax = result_dr_df.plot(y=['f1_scores', 'accuracy_scores'], style='x-', figsize=(60, 20), xticks=states)
plt.savefig('./result_dr.png')

# result_df_reduced = result_df[['f1_scores', 'accuracy_scores']]
# ax = result_df_reduced.plot(kind='bar', figsize=(40, 40))
# ax.set_xticklabels(states, rotation=45, fontsize=20)
# plt.savefig('results.png')
# fig, axs = plt.subplots(len(labels_clf), len(labels_dr), squeeze=False, figsize=(40, 20))
# counter = 0
# for i in range(len(labels_clf)):
#     for j in range(len(labels_dr)):
#         sns.heatmap(cms[counter], annot=True, ax=axs[i, j])
#         axs[i, j].set_title(states[counter], fontsize=20)
#         counter = counter + 1
# fig.savefig('confusion_matrix.png')
# fig, axs = plt.subplots(len(labels_clf), len(labels_dr), squeeze=False, figsize=(40, 20))
# counter = 0
# for i in range(len(labels_clf)):
#     for j in range(len(labels_dr)):
#         fpr, tpr, _ = rocs[counter]
#         axs[i, j].plot(fpr, tpr)
#         axs[i, j].set_title(states[counter], fontsize=20)
#         counter = counter + 1
# fig.savefig('roc_curves.png')
# --- dummy --- #
# # 01
# print(X.info(verbose=True, show_counts=True))
# print(Y.info(verbose=True, show_counts=True))
# print(sampled_data.info(verbose=True, show_counts=True))
# print(death_proportion)
# # 02
# pca = PCA()
# X_train_pca = pca.fit_transform(X_train)
# X_test_pca = pca.transform(X_test)
# explained_variance_s = pca.explained_variance_ratio_
# plt.figure(figsize=(10, 8))
# plt.plot(explained_variance_s)
# plt.show()
# nb_components = [2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 60, 70, 80, 90, 97]
# for i in nb_components:
#      pca = PCA(n_components=i)
#      X_train_pca = pca.fit_transform(X_train)
#      print('Total Explained Variance Ratio using {} components = {}%'.format(i, round(np.sum(pca.explained_variance_ratio_)*100, 2)))
# # 03
# result_df.insert(loc=0, column='Classifier', value=labels)


------------------------------------------------------------------------------------------------


from collections import OrderedDict
from functools import partial
from time import time
import math
from itertools import combinations

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.ticker import NullFormatter

from sklearn import manifold

import pandas as pd

################################## Data preprocessing ####################################

data = pd.read_csv("../data/dataset.csv")

# Removes empty feature
data.drop(['Unnamed: 83'], axis=1, inplace=True)

# Removes features not usefull
data.drop(['encounter_id', 'patient_id', 'icu_admit_source',
           'icu_id', 'icu_stay_type', 'icu_type'], axis=1, inplace=True)

# Removes rows with at least one null value in all features
data = data[data.isna().sum(axis=1) == 0]

# Correct typo of apache_2_bodysystem "Undefined diagnosis"
data["apache_2_bodysystem"].replace({"Undefined diagnoses": "Undefined Diagnoses"}, inplace=True)

# One hot encoding of string data
for (columnName, columnData) in data.iteritems():
    if columnData.dtype == "object":
        one_hot = pd.get_dummies(columnData, prefix=columnName)
        data.drop([columnName], axis=1, inplace=True)
        data = data.join(one_hot)

print(data.shape)
print(data.nunique())
print(data.info(verbose=True, show_counts=True))
print("No. of rows with missing values:", data.isnull().any(axis=1).sum())

################################### Reduction part ##################################

n_points = 2000
n_neighbors = 15
n_components = 4

total_death = data["hospital_death"]
n_death = len(total_death[total_death == 1])
n_survived = len(total_death) - n_death
death_proportion = n_death / len(total_death)
data_dead = data.loc[(data["hospital_death"] == 1)]
data_survived = data.loc[(data["hospital_death"] == 0)]

X = data_dead.sample(int(n_points * death_proportion))
Y = data_survived.sample(n_points - int(n_points * death_proportion))
sampled_data = pd.concat([X, Y])
Death = sampled_data["hospital_death"]
print(X.info(verbose=True, show_counts=True))
print(Y.info(verbose=True, show_counts=True))
print(sampled_data.info(verbose=True, show_counts=True))
print(death_proportion)

list_comb = list(range(n_components))
list_comb = list(combinations(list_comb, 2))
n_pairs = len(list_comb)

# Create figure
fig, axs = plt.subplots(n_pairs, 2, squeeze=False, figsize=(35, 18))
fig.suptitle(
    "Manifold Learning with %i points, %i neighbors" % (n_points, n_neighbors), fontsize=14
)

# Set-up manifold methods
LLE = partial(
    manifold.LocallyLinearEmbedding,
    n_neighbors=n_neighbors,
    n_components=n_components,
    eigen_solver="dense",
)

methods = OrderedDict()
methods["LLE"] = LLE(method="standard")
methods["Hessian LLE"] = LLE(method="hessian")

# Plot results
for m, (label, method) in enumerate(methods.items()):
    t0 = time()
    Y = method.fit_transform(sampled_data)
    t1 = time()
    print("%s: %.2g sec" % (label, t1 - t0))
    for (l, x) in enumerate(list_comb):
        axs[l, m].scatter(Y[Death == 0, x[0]], Y[Death == 0, x[1]], c="green", label="Survived")
        axs[l, m].scatter(Y[Death == 1, x[0]], Y[Death == 1, x[1]], c="red", label="Died")
        if l == 0:
            axs[l, m].set_title("%s (%.2g sec)" % (label, t1 - t0))
        axs[l, m].xaxis.set_major_formatter(NullFormatter())
        axs[l, m].yaxis.set_major_formatter(NullFormatter())
        axs[l, m].axis("tight")
        axs[l, m].legend()
        axs[l, m].set_xlabel(f"dim : {x[0]}")
        axs[l, m].set_ylabel(f"dim : {x[1]}")

plt.show()
fig.savefig("fig.png")


--------------------------------------------------------------------------------------------


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import plotly.express as px
import plotly.graph_objs as go
from plotly.subplots import make_subplots

import warnings

warnings.filterwarnings('ignore')

# import data
data = pd.read_csv("../data/dataset.csv")

print(data.shape)
print(data.head())
print(data.nunique())
print(data.info(verbose=True, show_counts=True))
print(data.describe())

# Removes an empty feature
data = data.drop(['Unnamed: 83'], axis=1)

print(data.shape)
print("No. of rows with missing values:", data.isnull().any(axis=1).sum())
print(data.columns)

# Removes features not usefull
data.drop(['encounter_id', 'patient_id', 'icu_admit_source',
           'icu_id', 'icu_stay_type', 'icu_type'], axis=1, inplace=True)

# Removes null values in those features
data = data[data[['bmi', 'weight', 'height']].isna().sum(axis=1) == 0]

############################################### Plots ##############################################
fig = px.histogram(data[['age', 'gender', 'hospital_death', 'bmi']].dropna(), x="age", y="hospital_death",
                   color='gender',
                   marginal='box', hover_data=data[['age', 'gender', 'hospital_death', 'bmi']].columns)
fig.show()

unpivot = pd.melt(data, data.describe().columns[0], data.describe().columns[1:])

fig2 = sns.FacetGrid(unpivot, col='variable', col_wrap=3,
                     sharex=False, sharey=False)
fig2.map(sns.kdeplot, "value")
plt.show()

weight_data = data[['weight', 'hospital_death', 'bmi']]
weight_data['weight'] = weight_data['weight'].round(0)
weight_data['bmi'] = weight_data['bmi'].round(0)
weight_death = weight_data[['weight', 'hospital_death']].groupby('weight').mean().reset_index()
bmi_death = weight_data[['bmi', 'hospital_death']].groupby('bmi').mean().reset_index()
fig = make_subplots(rows=1, cols=2, shared_yaxes=True)
fig.add_trace(
    go.Scatter(x=weight_death['weight'], y=weight_death['hospital_death'], name="Weight"),
    row=1, col=1
)
fig.add_trace(
    go.Scatter(x=bmi_death['bmi'], y=bmi_death['hospital_death'], name="BMI"),
    row=1, col=2
)
fig.update_layout(
    title_text="<b>impacts of BMI and weight over patients<b>"
)
fig.update_yaxes(title_text="<b>Average Hospital Death")
fig.show()


-------------------------------------------------------------------------------------------



